{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34f98c97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amr20\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openml\n",
    "import data_preprocess as dp\n",
    "\n",
    "data_name = 'bank32nh'\n",
    "\n",
    "data = openml.datasets.get_dataset(833)\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = data.get_data(\n",
    "    target=data.default_target_attribute, dataset_format=\"dataframe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a854335e",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83124ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1cx</th>\n",
       "      <th>a1cy</th>\n",
       "      <th>a1sx</th>\n",
       "      <th>a1sy</th>\n",
       "      <th>a1rho</th>\n",
       "      <th>a1pop</th>\n",
       "      <th>a2cx</th>\n",
       "      <th>a2cy</th>\n",
       "      <th>a2sx</th>\n",
       "      <th>a2sy</th>\n",
       "      <th>...</th>\n",
       "      <th>b1eff</th>\n",
       "      <th>b2x</th>\n",
       "      <th>b2y</th>\n",
       "      <th>b2call</th>\n",
       "      <th>b2eff</th>\n",
       "      <th>b3x</th>\n",
       "      <th>b3y</th>\n",
       "      <th>b3call</th>\n",
       "      <th>b3eff</th>\n",
       "      <th>mxql</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.413010</td>\n",
       "      <td>0.607442</td>\n",
       "      <td>0.332608</td>\n",
       "      <td>0.406812</td>\n",
       "      <td>-0.151224</td>\n",
       "      <td>1.525222</td>\n",
       "      <td>-0.144368</td>\n",
       "      <td>0.852368</td>\n",
       "      <td>0.412397</td>\n",
       "      <td>1.728169</td>\n",
       "      <td>...</td>\n",
       "      <td>1.974706</td>\n",
       "      <td>-0.776759</td>\n",
       "      <td>-0.783770</td>\n",
       "      <td>8</td>\n",
       "      <td>0.603486</td>\n",
       "      <td>-0.997118</td>\n",
       "      <td>-0.502138</td>\n",
       "      <td>5</td>\n",
       "      <td>1.169388</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.602384</td>\n",
       "      <td>0.350618</td>\n",
       "      <td>0.429196</td>\n",
       "      <td>0.414476</td>\n",
       "      <td>-0.124489</td>\n",
       "      <td>4.597991</td>\n",
       "      <td>0.579458</td>\n",
       "      <td>0.651134</td>\n",
       "      <td>0.104394</td>\n",
       "      <td>0.636356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798979</td>\n",
       "      <td>-0.002820</td>\n",
       "      <td>-0.080542</td>\n",
       "      <td>2</td>\n",
       "      <td>1.125542</td>\n",
       "      <td>-0.983397</td>\n",
       "      <td>-0.107632</td>\n",
       "      <td>5</td>\n",
       "      <td>1.186039</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.322881</td>\n",
       "      <td>-0.538491</td>\n",
       "      <td>1.602260</td>\n",
       "      <td>0.039605</td>\n",
       "      <td>0.196023</td>\n",
       "      <td>1.909005</td>\n",
       "      <td>-0.675672</td>\n",
       "      <td>0.963618</td>\n",
       "      <td>0.147458</td>\n",
       "      <td>1.414008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577355</td>\n",
       "      <td>-0.952645</td>\n",
       "      <td>-0.571600</td>\n",
       "      <td>5</td>\n",
       "      <td>1.280392</td>\n",
       "      <td>0.771129</td>\n",
       "      <td>-0.665756</td>\n",
       "      <td>5</td>\n",
       "      <td>1.024203</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.233570</td>\n",
       "      <td>-0.936451</td>\n",
       "      <td>1.710192</td>\n",
       "      <td>2.179527</td>\n",
       "      <td>0.438461</td>\n",
       "      <td>4.742055</td>\n",
       "      <td>-0.163625</td>\n",
       "      <td>-0.923273</td>\n",
       "      <td>0.597622</td>\n",
       "      <td>0.118409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760084</td>\n",
       "      <td>-0.198235</td>\n",
       "      <td>-0.205276</td>\n",
       "      <td>2</td>\n",
       "      <td>0.509727</td>\n",
       "      <td>-0.579544</td>\n",
       "      <td>0.480094</td>\n",
       "      <td>6</td>\n",
       "      <td>1.568492</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.403126</td>\n",
       "      <td>0.313367</td>\n",
       "      <td>0.822382</td>\n",
       "      <td>1.393975</td>\n",
       "      <td>0.253435</td>\n",
       "      <td>9.398630</td>\n",
       "      <td>0.312528</td>\n",
       "      <td>0.288321</td>\n",
       "      <td>0.431867</td>\n",
       "      <td>0.110369</td>\n",
       "      <td>...</td>\n",
       "      <td>1.170067</td>\n",
       "      <td>0.573352</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>2</td>\n",
       "      <td>0.622033</td>\n",
       "      <td>-0.134747</td>\n",
       "      <td>0.669948</td>\n",
       "      <td>3</td>\n",
       "      <td>1.295913</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>0.574602</td>\n",
       "      <td>0.838725</td>\n",
       "      <td>0.413666</td>\n",
       "      <td>0.106538</td>\n",
       "      <td>-0.234311</td>\n",
       "      <td>1.826603</td>\n",
       "      <td>-0.812191</td>\n",
       "      <td>-0.641247</td>\n",
       "      <td>0.032074</td>\n",
       "      <td>0.311381</td>\n",
       "      <td>...</td>\n",
       "      <td>1.770834</td>\n",
       "      <td>0.458032</td>\n",
       "      <td>-0.252926</td>\n",
       "      <td>2</td>\n",
       "      <td>1.300314</td>\n",
       "      <td>0.265640</td>\n",
       "      <td>0.075141</td>\n",
       "      <td>2</td>\n",
       "      <td>1.061276</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8188</th>\n",
       "      <td>-0.328542</td>\n",
       "      <td>0.431295</td>\n",
       "      <td>1.871342</td>\n",
       "      <td>0.195284</td>\n",
       "      <td>-0.454419</td>\n",
       "      <td>1.266566</td>\n",
       "      <td>0.074520</td>\n",
       "      <td>-0.233416</td>\n",
       "      <td>0.967483</td>\n",
       "      <td>0.902844</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019917</td>\n",
       "      <td>-0.943163</td>\n",
       "      <td>0.184895</td>\n",
       "      <td>3</td>\n",
       "      <td>0.760308</td>\n",
       "      <td>0.184409</td>\n",
       "      <td>0.313283</td>\n",
       "      <td>2</td>\n",
       "      <td>1.790066</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8189</th>\n",
       "      <td>0.941521</td>\n",
       "      <td>-0.820731</td>\n",
       "      <td>1.055311</td>\n",
       "      <td>0.186861</td>\n",
       "      <td>-0.184692</td>\n",
       "      <td>2.716994</td>\n",
       "      <td>0.965042</td>\n",
       "      <td>0.328241</td>\n",
       "      <td>0.138787</td>\n",
       "      <td>0.502774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847430</td>\n",
       "      <td>0.935832</td>\n",
       "      <td>0.289506</td>\n",
       "      <td>5</td>\n",
       "      <td>1.714573</td>\n",
       "      <td>0.216543</td>\n",
       "      <td>-0.763388</td>\n",
       "      <td>6</td>\n",
       "      <td>1.153544</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8190</th>\n",
       "      <td>0.206743</td>\n",
       "      <td>0.916655</td>\n",
       "      <td>0.265062</td>\n",
       "      <td>0.666585</td>\n",
       "      <td>0.027745</td>\n",
       "      <td>5.556948</td>\n",
       "      <td>-0.869593</td>\n",
       "      <td>-0.703412</td>\n",
       "      <td>2.032233</td>\n",
       "      <td>0.779990</td>\n",
       "      <td>...</td>\n",
       "      <td>1.199289</td>\n",
       "      <td>-0.104562</td>\n",
       "      <td>0.752754</td>\n",
       "      <td>6</td>\n",
       "      <td>0.561357</td>\n",
       "      <td>0.908959</td>\n",
       "      <td>-0.137524</td>\n",
       "      <td>5</td>\n",
       "      <td>1.392110</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8191</th>\n",
       "      <td>0.078102</td>\n",
       "      <td>-0.588395</td>\n",
       "      <td>2.976553</td>\n",
       "      <td>0.257401</td>\n",
       "      <td>0.243605</td>\n",
       "      <td>3.471094</td>\n",
       "      <td>0.966461</td>\n",
       "      <td>0.119917</td>\n",
       "      <td>0.676696</td>\n",
       "      <td>1.228387</td>\n",
       "      <td>...</td>\n",
       "      <td>1.097475</td>\n",
       "      <td>0.089226</td>\n",
       "      <td>-0.545795</td>\n",
       "      <td>7</td>\n",
       "      <td>1.666457</td>\n",
       "      <td>-0.813706</td>\n",
       "      <td>-0.049569</td>\n",
       "      <td>4</td>\n",
       "      <td>1.786096</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8192 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          a1cx      a1cy      a1sx      a1sy     a1rho     a1pop      a2cx  \\\n",
       "0     0.413010  0.607442  0.332608  0.406812 -0.151224  1.525222 -0.144368   \n",
       "1    -0.602384  0.350618  0.429196  0.414476 -0.124489  4.597991  0.579458   \n",
       "2    -0.322881 -0.538491  1.602260  0.039605  0.196023  1.909005 -0.675672   \n",
       "3    -0.233570 -0.936451  1.710192  2.179527  0.438461  4.742055 -0.163625   \n",
       "4     0.403126  0.313367  0.822382  1.393975  0.253435  9.398630  0.312528   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8187  0.574602  0.838725  0.413666  0.106538 -0.234311  1.826603 -0.812191   \n",
       "8188 -0.328542  0.431295  1.871342  0.195284 -0.454419  1.266566  0.074520   \n",
       "8189  0.941521 -0.820731  1.055311  0.186861 -0.184692  2.716994  0.965042   \n",
       "8190  0.206743  0.916655  0.265062  0.666585  0.027745  5.556948 -0.869593   \n",
       "8191  0.078102 -0.588395  2.976553  0.257401  0.243605  3.471094  0.966461   \n",
       "\n",
       "          a2cy      a2sx      a2sy  ...     b1eff       b2x       b2y  b2call  \\\n",
       "0     0.852368  0.412397  1.728169  ...  1.974706 -0.776759 -0.783770       8   \n",
       "1     0.651134  0.104394  0.636356  ...  0.798979 -0.002820 -0.080542       2   \n",
       "2     0.963618  0.147458  1.414008  ...  0.577355 -0.952645 -0.571600       5   \n",
       "3    -0.923273  0.597622  0.118409  ...  0.760084 -0.198235 -0.205276       2   \n",
       "4     0.288321  0.431867  0.110369  ...  1.170067  0.573352  0.315217       2   \n",
       "...        ...       ...       ...  ...       ...       ...       ...     ...   \n",
       "8187 -0.641247  0.032074  0.311381  ...  1.770834  0.458032 -0.252926       2   \n",
       "8188 -0.233416  0.967483  0.902844  ...  1.019917 -0.943163  0.184895       3   \n",
       "8189  0.328241  0.138787  0.502774  ...  0.847430  0.935832  0.289506       5   \n",
       "8190 -0.703412  2.032233  0.779990  ...  1.199289 -0.104562  0.752754       6   \n",
       "8191  0.119917  0.676696  1.228387  ...  1.097475  0.089226 -0.545795       7   \n",
       "\n",
       "         b2eff       b3x       b3y  b3call     b3eff  mxql  \n",
       "0     0.603486 -0.997118 -0.502138       5  1.169388     9  \n",
       "1     1.125542 -0.983397 -0.107632       5  1.186039     7  \n",
       "2     1.280392  0.771129 -0.665756       5  1.024203     6  \n",
       "3     0.509727 -0.579544  0.480094       6  1.568492     7  \n",
       "4     0.622033 -0.134747  0.669948       3  1.295913     9  \n",
       "...        ...       ...       ...     ...       ...   ...  \n",
       "8187  1.300314  0.265640  0.075141       2  1.061276     7  \n",
       "8188  0.760308  0.184409  0.313283       2  1.790066     7  \n",
       "8189  1.714573  0.216543 -0.763388       6  1.153544     7  \n",
       "8190  0.561357  0.908959 -0.137524       5  1.392110     9  \n",
       "8191  1.666457 -0.813706 -0.049569       4  1.786096     8  \n",
       "\n",
       "[8192 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "nominal = [b for a, b in zip(categorical_indicator, attribute_names) if a]\n",
    "numerical = [b for a, b in zip(categorical_indicator, attribute_names) if not a]\n",
    "\n",
    "'''\n",
    "encoded_data = pd.get_dummies(X, columns=nominal)\n",
    "columns = encoded_data.columns\n",
    "updates = {}\n",
    "for col in columns:\n",
    "    if any(x in col for x in set((',', '[', ']', '<', '>'))):\n",
    "        updates[col] = col.replace(',', '_').replace('[', '_').replace(']', '_').replace('<', 'less').replace('>', 'greater')\n",
    "encoded_data = encoded_data.rename(columns=updates)\n",
    "\n",
    "encoded_data.fillna(0, inplace=True)\n",
    "encoded_data\n",
    "'''\n",
    "\n",
    "encoded_data = deepcopy(X)\n",
    "\n",
    "for col in nominal:\n",
    "    \n",
    "    mapping = {c: i+1 for i, c in enumerate(encoded_data[col].unique())}\n",
    "    encoded_data[col] = encoded_data[col].replace(mapping)\n",
    "    try:\n",
    "        encoded_data[col] = encoded_data[col].cat.add_categories([0])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "encoded_data = encoded_data[numerical + nominal]\n",
    "encoded_data.fillna(0, inplace=True)\n",
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "354c14a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['a1cx',\n",
       "  'a1cy',\n",
       "  'a1sx',\n",
       "  'a1sy',\n",
       "  'a1rho',\n",
       "  'a1pop',\n",
       "  'a2cx',\n",
       "  'a2cy',\n",
       "  'a2sx',\n",
       "  'a2sy',\n",
       "  'a2rho',\n",
       "  'a2pop',\n",
       "  'a3cx',\n",
       "  'a3cy',\n",
       "  'a3sx',\n",
       "  'a3sy',\n",
       "  'a3rho',\n",
       "  'a3pop',\n",
       "  'temp',\n",
       "  'b1x',\n",
       "  'b1y',\n",
       "  'b1call',\n",
       "  'b1eff',\n",
       "  'b2x',\n",
       "  'b2y',\n",
       "  'b2call',\n",
       "  'b2eff',\n",
       "  'b3x',\n",
       "  'b3y',\n",
       "  'b3call',\n",
       "  'b3eff',\n",
       "  'mxql'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nominal, numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c57b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       0\n",
       "3       1\n",
       "4       0\n",
       "       ..\n",
       "8187    0\n",
       "8188    0\n",
       "8189    0\n",
       "8190    0\n",
       "8191    0\n",
       "Name: binaryClass, Length: 8192, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {v: i for i, v in enumerate(y.unique())}\n",
    "\n",
    "\n",
    "y = y.replace(mapping)\n",
    "#y = pd.get_dummies(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747e1b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (5734, 32)\n",
      " test shape: (1229, 32)\n",
      " dev shape: (1229, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, left_out, train_label, y_left_out = train_test_split(encoded_data, y, test_size=0.3, random_state=42)\n",
    "test_data, dev_data, test_label, dev_label = train_test_split(left_out, y_left_out, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f'train shape: {train_data.shape}\\n test shape: {test_data.shape}\\n dev shape: {dev_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c709d0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label.unique()), len(test_label.unique()), len(dev_label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa7fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "\n",
    "nn = StandardScaler()\n",
    "\n",
    "nn.fit(train_data[numerical])\n",
    "\n",
    "train_data[numerical] = nn.transform(train_data[numerical])\n",
    "dev_data[numerical] = nn.transform(dev_data[numerical])\n",
    "test_data[numerical] = nn.transform(test_data[numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba008d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_over, y_over = oversample.fit_resample(train_data, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1d08a",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde976a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amr20\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.80686 | val_0_auc: 0.56638 |  0:00:02s\n",
      "epoch 1  | loss: 0.6677  | val_0_auc: 0.67285 |  0:00:02s\n",
      "epoch 2  | loss: 0.62477 | val_0_auc: 0.73003 |  0:00:03s\n",
      "epoch 3  | loss: 0.59407 | val_0_auc: 0.76519 |  0:00:03s\n",
      "epoch 4  | loss: 0.56633 | val_0_auc: 0.78314 |  0:00:04s\n",
      "epoch 5  | loss: 0.54817 | val_0_auc: 0.79652 |  0:00:05s\n",
      "epoch 6  | loss: 0.52692 | val_0_auc: 0.81711 |  0:00:05s\n",
      "epoch 7  | loss: 0.51533 | val_0_auc: 0.82566 |  0:00:06s\n",
      "epoch 8  | loss: 0.49948 | val_0_auc: 0.83238 |  0:00:06s\n",
      "epoch 9  | loss: 0.48587 | val_0_auc: 0.84632 |  0:00:07s\n",
      "epoch 10 | loss: 0.4745  | val_0_auc: 0.85582 |  0:00:08s\n",
      "epoch 11 | loss: 0.46335 | val_0_auc: 0.84801 |  0:00:08s\n",
      "epoch 12 | loss: 0.45092 | val_0_auc: 0.85772 |  0:00:09s\n",
      "epoch 13 | loss: 0.45214 | val_0_auc: 0.85558 |  0:00:09s\n",
      "epoch 14 | loss: 0.43975 | val_0_auc: 0.87061 |  0:00:10s\n",
      "epoch 15 | loss: 0.43137 | val_0_auc: 0.87054 |  0:00:10s\n",
      "epoch 16 | loss: 0.42809 | val_0_auc: 0.8676  |  0:00:11s\n",
      "epoch 17 | loss: 0.4334  | val_0_auc: 0.86678 |  0:00:12s\n",
      "epoch 18 | loss: 0.42152 | val_0_auc: 0.86599 |  0:00:12s\n",
      "epoch 19 | loss: 0.41598 | val_0_auc: 0.87216 |  0:00:13s\n",
      "epoch 20 | loss: 0.41726 | val_0_auc: 0.87701 |  0:00:13s\n",
      "epoch 21 | loss: 0.41071 | val_0_auc: 0.87321 |  0:00:14s\n",
      "epoch 22 | loss: 0.40462 | val_0_auc: 0.87351 |  0:00:14s\n",
      "epoch 23 | loss: 0.40666 | val_0_auc: 0.87509 |  0:00:15s\n",
      "epoch 24 | loss: 0.40526 | val_0_auc: 0.87687 |  0:00:16s\n",
      "epoch 25 | loss: 0.40028 | val_0_auc: 0.88014 |  0:00:16s\n",
      "epoch 26 | loss: 0.38865 | val_0_auc: 0.88313 |  0:00:17s\n",
      "epoch 27 | loss: 0.39822 | val_0_auc: 0.8818  |  0:00:17s\n",
      "epoch 28 | loss: 0.39712 | val_0_auc: 0.87737 |  0:00:18s\n",
      "epoch 29 | loss: 0.3942  | val_0_auc: 0.87758 |  0:00:18s\n",
      "epoch 30 | loss: 0.39589 | val_0_auc: 0.87844 |  0:00:19s\n",
      "epoch 31 | loss: 0.388   | val_0_auc: 0.88209 |  0:00:20s\n",
      "epoch 32 | loss: 0.39237 | val_0_auc: 0.88157 |  0:00:20s\n",
      "epoch 33 | loss: 0.3797  | val_0_auc: 0.88309 |  0:00:21s\n",
      "epoch 34 | loss: 0.38081 | val_0_auc: 0.88431 |  0:00:21s\n",
      "epoch 35 | loss: 0.37407 | val_0_auc: 0.88187 |  0:00:22s\n",
      "epoch 36 | loss: 0.37802 | val_0_auc: 0.88375 |  0:00:23s\n",
      "epoch 37 | loss: 0.37251 | val_0_auc: 0.88496 |  0:00:23s\n",
      "epoch 38 | loss: 0.37956 | val_0_auc: 0.88369 |  0:00:24s\n",
      "epoch 39 | loss: 0.37302 | val_0_auc: 0.88271 |  0:00:24s\n",
      "epoch 40 | loss: 0.36655 | val_0_auc: 0.88433 |  0:00:25s\n",
      "epoch 41 | loss: 0.37081 | val_0_auc: 0.88158 |  0:00:25s\n",
      "epoch 42 | loss: 0.3606  | val_0_auc: 0.88365 |  0:00:26s\n",
      "epoch 43 | loss: 0.37355 | val_0_auc: 0.88435 |  0:00:27s\n",
      "epoch 44 | loss: 0.35406 | val_0_auc: 0.88224 |  0:00:27s\n",
      "epoch 45 | loss: 0.35898 | val_0_auc: 0.88246 |  0:00:28s\n",
      "epoch 46 | loss: 0.35531 | val_0_auc: 0.88123 |  0:00:28s\n",
      "epoch 47 | loss: 0.36004 | val_0_auc: 0.88145 |  0:00:29s\n",
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 37 and best_val_0_auc = 0.88496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amr20\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc is : 0.85252810773042\n",
      "prec 0.7390200642944241\n",
      "recall 0.7739249190116242\n",
      "f-score 0.7471898720463003\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "clf = TabNetClassifier()\n",
    "clf.fit(\n",
    "  X_over.values, y_over.values,\n",
    "  eval_set=[(dev_data.values, dev_label.values)],\n",
    "  patience = 10\n",
    ")\n",
    "preds = clf.predict(test_data.values)\n",
    "\n",
    "rf_pred_prob = [i[1] for i in clf.predict_proba(test_data.values)]\n",
    "\n",
    "roc = roc_auc_score(\n",
    "    test_label,\n",
    "    rf_pred_prob,\n",
    ")\n",
    "prec = precision_score(test_label.tolist(), preds.tolist(), average='macro')\n",
    "recall = recall_score(test_label.tolist(), preds.tolist(), average='macro')\n",
    "f_score = f1_score(test_label.tolist(), preds.tolist(), average='macro')\n",
    "\n",
    "\n",
    "print('roc is : {}\\nprec {}\\nrecall {}\\nf-score {}' .format(roc, prec, recall, f_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d144549c",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dca035e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB test_acc: 0.8014646053702197\n",
      "recall: 0.7372340087658007\n",
      "precision: 0.7668134764997107\n",
      "f_score: 0.7487364596739596\n",
      "\n",
      "ROC: 0.8759575684431176\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "from tqdm import tqdm\n",
    " \n",
    "comb_data = pd.concat([X_over, dev_data])\n",
    "comb_labels = pd.concat([y_over, dev_label])\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=0).fit(comb_data, comb_labels)\n",
    "\n",
    "rf_pred = rf_clf.predict(test_data)\n",
    "test_ac = rf_clf.score(test_data, test_label)\n",
    "f_score = f1_score(test_label, rf_pred, average='macro')\n",
    "prec = precision_score(test_label, rf_pred, average='macro')\n",
    "recall = recall_score(test_label, rf_pred, average='macro')\n",
    "\n",
    "print(f'XGB test_acc: {test_ac}\\nrecall: {recall}\\nprecision: {prec}\\nf_score: {f_score}\\n')\n",
    "rf_pred_prob = [i[1] for i in rf_clf.predict_proba(test_data)]\n",
    "\n",
    "weighted_roc = roc_auc_score(\n",
    "    test_label,\n",
    "    rf_pred_prob\n",
    ")\n",
    "\n",
    "print(f'ROC: {weighted_roc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9196ea7e",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87e27cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:02:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGB test_acc: 0.8087876322213181\n",
      "recall: 0.7686908467255288\n",
      "precision: 0.7708673518394006\n",
      "f_score: 0.769759367987213\n",
      " ROC: 0.874591882106333\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "xg_boost = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False)\n",
    "\n",
    "comb_data = pd.concat([X_over, dev_data])\n",
    "comb_labels = pd.concat([y_over, dev_label])\n",
    "\n",
    "xgb_clf = xg_boost.fit(comb_data.values.astype(float), comb_labels)\n",
    "test_acc = xgb_clf.score(test_data.values.astype(float), test_label)\n",
    "test_pred = xgb_clf.predict(test_data.values.astype(float))\n",
    "f_score = f1_score(test_label, test_pred, average='macro')\n",
    "prec = precision_score(test_label, test_pred, average='macro')\n",
    "recall = recall_score(test_label, test_pred, average='macro')\n",
    "\n",
    "pred_score = [i[1] for i in xgb_clf.predict_proba(test_data.values.astype(float))]\n",
    "weighted_roc = roc_auc_score(\n",
    "    test_label,\n",
    "    pred_score,\n",
    ")\n",
    "\n",
    "print(f'XGB test_acc: {test_acc}\\nrecall: {recall}\\nprecision: {prec}\\nf_score: {f_score}\\n ROC: {weighted_roc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7178908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
